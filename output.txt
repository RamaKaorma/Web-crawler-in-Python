for link in links:
    full_url = urljoin(seed_url, link['href'])
    sub_url = urlparse(link['href'])
    sub_path = sub_url.path
    if link not in seed_links and "href" in link.attrs:
        if not check_link_ok(robot_rules, link['href']) or not check_link_ok(robot_rules, sub_path):
            continue
        to_visit.append(full_url)

while (to_visit):
    # Impose a limit to avoid breaking the site
    if pages_visited == SAFE_PAGE_LIMIT or 'href' not in link.attrs:
        break
    
    # Consume the list of urls
    # Assign the first item from to_visit to link, and remove it from the to_visit list
        # to make sure each link is visited once
    link = to_visit.pop(0)
    # Get the webpage
    page = requests.get(link)
    if not page.ok:
        continue
    # Scrape the code, aka, get the html to pull out the links in the page
    soup = bs4.BeautifulSoup(page.text, 'html.parser')

    # Mark the item as visited, i.e., add to visited dict, remove from to_visit
    visited[link] = True
    new_links = soup.findAll('a')
    for new_link in new_links:
        # Skip the links that don't have href values (links that don't actually exist or don't lead anywhere)
        if "href" not in new_link.attrs:
            continue

        new_item = new_link['href']
        if '#' in new_item:
            index = new_item.index('#')
            new_item = new_item[:index]
        
        # Skip any links which Wikipedia has asked us not to visit.
        if not check_link_ok(robot_rules, new_item):
            continue

        # Need to concat with base_url to get an absolute link, 
        # an example item <a href="/wiki/Category:Marvel_Cinematic_Universe_images_by_film_series"> 
        new_url = urljoin(link, new_item)
        # print("new_url      " + new_url)
        # Check it's not already in the list before adding it  and (new_parsed_url.netloc == base_url).
        if new_url not in visited and new_url not in to_visit:
            to_visit.append(new_url)
        
    # Increase the number of pages we've visited so the page limit is enforced.
    pages_visited = pages_visited + 1
    